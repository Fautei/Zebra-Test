{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403ea010",
   "metadata": {},
   "source": [
    "# Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbe23e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torch-2.7.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchvision-0.22.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.7.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.61 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.57 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.57 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.7.1.26 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.3.14 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.41 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.55 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.2.55 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.7.53 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.55 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.61 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.0.11 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting setuptools>=40.8.0 (from triton==3.3.1->torch)\n",
      "  Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torch-2.7.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (1039.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl (609.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl (726.9 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl (260.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (292.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached https://download.pytorch.org/whl/triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torchvision-0.22.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (8.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.7.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/28\u001b[0m [torchaudio]237m━\u001b[0m \u001b[32m27/28\u001b[0m [torchaudio]12]e-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.8.3.14 nvidia-cuda-cupti-cu12-12.8.57 nvidia-cuda-nvrtc-cu12-12.8.61 nvidia-cuda-runtime-cu12-12.8.57 nvidia-cudnn-cu12-9.7.1.26 nvidia-cufft-cu12-11.3.3.41 nvidia-cufile-cu12-1.13.0.11 nvidia-curand-cu12-10.3.9.55 nvidia-cusolver-cu12-11.7.2.55 nvidia-cusparse-cu12-12.5.7.53 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.8.55 pillow-11.0.0 setuptools-70.2.0 sympy-1.13.3 torch-2.7.1+cu128 torchaudio-2.7.1+cu128 torchvision-0.22.1+cu128 triton-3.3.1\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.159-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.10/site-packages (from opencv-python) (2.1.2)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pyyaml (from wandb)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests<3,>=2.0.0 (from wandb)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.31.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./.venv/lib/python3.10/site-packages (from wandb) (4.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.0.0->wandb)\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.0.0->wandb)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0.0->wandb)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.0.0->wandb)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting matplotlib>=3.3.0 (from ultralytics)\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./.venv/lib/python3.10/site-packages (from ultralytics) (11.0.0)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./.venv/lib/python3.10/site-packages (from ultralytics) (2.7.1+cu128)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./.venv/lib/python3.10/site-packages (from ultralytics) (0.22.1+cu128)\n",
      "Collecting tqdm>=4.64.0 (from ultralytics)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Using cached pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.8.0->ultralytics) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading ultralytics-8.3.159-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Downloading sentry_sdk-2.31.0-py2.py3-none-any.whl (355 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Installing collected packages: pytz, py-cpuinfo, urllib3, tzdata, typing-inspection, tqdm, smmap, setproctitle, scipy, pyyaml, pyparsing, pydantic-core, protobuf, opencv-python, kiwisolver, idna, fonttools, cycler, contourpy, click, charset_normalizer, certifi, annotated-types, sentry-sdk, requests, pydantic, pandas, matplotlib, gitdb, gitpython, wandb, ultralytics-thop, ultralytics\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [ultralytics]37m━\u001b[0m \u001b[32m32/33\u001b[0m [ultralytics]thop]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 certifi-2025.6.15 charset_normalizer-3.4.2 click-8.2.1 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 kiwisolver-1.4.8 matplotlib-3.10.3 opencv-python-4.11.0.86 pandas-2.3.0 protobuf-6.31.1 py-cpuinfo-9.0.0 pydantic-2.11.7 pydantic-core-2.33.2 pyparsing-3.2.3 pytz-2025.2 pyyaml-6.0.2 requests-2.32.4 scipy-1.15.3 sentry-sdk-2.31.0 setproctitle-1.3.6 smmap-5.0.2 tqdm-4.67.1 typing-inspection-0.4.1 tzdata-2025.2 ultralytics-8.3.159 ultralytics-thop-2.0.14 urllib3-2.5.0 wandb-0.20.1\n",
      "✅ Updated 'wandb=True'\n",
      "JSONDict(\"/home/maxim/.config/Ultralytics/settings.json\"):\n",
      "{\n",
      "  \"settings_version\": \"0.0.6\",\n",
      "  \"datasets_dir\": \"/home/maxim/Desktop/car_recognition/datasets\",\n",
      "  \"weights_dir\": \"weights\",\n",
      "  \"runs_dir\": \"runs\",\n",
      "  \"uuid\": \"e3657e37c9ca46c701046a141bc6cedf304dffebc49578ba0649517c81b06639\",\n",
      "  \"sync\": true,\n",
      "  \"api_key\": \"\",\n",
      "  \"openai_api_key\": \"\",\n",
      "  \"clearml\": true,\n",
      "  \"comet\": true,\n",
      "  \"dvc\": true,\n",
      "  \"hub\": true,\n",
      "  \"mlflow\": true,\n",
      "  \"neptune\": true,\n",
      "  \"raytune\": true,\n",
      "  \"tensorboard\": false,\n",
      "  \"wandb\": true,\n",
      "  \"vscode_msg\": true,\n",
      "  \"openvino_msg\": true\n",
      "}\n",
      "💡 Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "! pip install opencv-python wandb ultralytics \n",
    "\n",
    "# Я использую weights and biases для мониторинга экспериментов\n",
    "! yolo settings wandb=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118abcff",
   "metadata": {},
   "source": [
    "# Извлечение изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9837ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'dish', 1: 'drink', 2: 'silverware', 3: 'garbage/other'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = [\"1.MOV\", \"2_1.MOV\", \"4.MOV\"]\n",
    "val = [\"3_1.MOV\"]\n",
    "test = [\"3_2.MOV\"]\n",
    "\n",
    "path_to_videos = Path(\"data/videos_raw\")\n",
    "dataset_path = Path(\"data/dataset\")\n",
    "\n",
    "with open(dataset_path/\"data.yaml\", 'r') as f:\n",
    "    dataset_meta = yaml.safe_load(f)\n",
    "    # CVAT все еще не может нормально экспортировать yolo формат(\n",
    "with open(dataset_path/\"data.yaml\", 'w') as f:\n",
    "    dataset_meta[\"train\"] = \"images/Train\"\n",
    "    dataset_meta[\"val\"] = \"images/Validation\"\n",
    "    dataset_meta[\"test\"] = \"images/Test\"\n",
    "    dataset_meta[\"path\"] = \"data/dataset\"\n",
    "    yaml.safe_dump(dataset_meta, f)\n",
    "\n",
    "classes = dataset_meta[\"names\"]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_yolo_labels(label_path):\n",
    "    labels = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue  # пропускаем неправильные строки\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            labels.append({\n",
    "                'class_id': class_id,\n",
    "                'x_center': x_center,\n",
    "                'y_center': y_center,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "    return labels\n",
    "\n",
    "def save_yolo_labels(label_path, labels):\n",
    "    with open(label_path, 'w') as f:\n",
    "        for label in labels:\n",
    "            line = f\"{label['class_id']} {label['x_center']:.6f} {label['y_center']:.6f} {label['width']:.6f} {label['height']:.6f}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "def flip_frame_and_labels(frame, labels, horizontal=False, vertical=False):\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Отразим изображение\n",
    "    if horizontal and vertical:\n",
    "        frame = cv2.flip(frame, -1)\n",
    "    elif horizontal:\n",
    "        frame = cv2.flip(frame, 1)\n",
    "    elif vertical:\n",
    "        frame = cv2.flip(frame, 0)\n",
    "\n",
    "    # Отразим лейблы\n",
    "    flipped_labels = []\n",
    "    for label in labels:\n",
    "        x = label['x_center']\n",
    "        y = label['y_center']\n",
    "\n",
    "        if horizontal:\n",
    "            x = 1.0 - x\n",
    "        if vertical:\n",
    "            y = 1.0 - y\n",
    "\n",
    "        flipped_labels.append({\n",
    "            'class_id': label['class_id'],\n",
    "            'x_center': x,\n",
    "            'y_center': y,\n",
    "            'width': label['width'],\n",
    "            'height': label['height']\n",
    "        })\n",
    "\n",
    "    return frame, flipped_labels\n",
    "\n",
    "def draw_boxes(img, labels, class_names):\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    def get_color(class_id):\n",
    "        return (30*class_id % 255, 70*class_id % 255, abs(255 - 50 * class_id) % 255)\n",
    "\n",
    "    for label in labels:\n",
    "        # Преобразуем нормализованные координаты в пиксели\n",
    "        x_center = label['x_center'] * img_w\n",
    "        y_center = label['y_center'] * img_h\n",
    "        width = label['width'] * img_w\n",
    "        height = label['height'] * img_h\n",
    "\n",
    "        # Координаты бокса (левая верхняя и правая нижняя точки)\n",
    "        x1 = int(x_center - width / 2)\n",
    "        y1 = int(y_center - height / 2)\n",
    "        x2 = int(x_center + width / 2)\n",
    "        y2 = int(y_center + height / 2)\n",
    "        class_id = label['class_id']\n",
    "        color = get_color(class_id)\n",
    "        # Рисуем прямоугольник\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Подпись с названием класса\n",
    "        label_text = class_names[class_id] if class_names else str(class_id)\n",
    "        cv2.putText(img, label_text, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "for name, videos in tqdm([(\"Train\", train), (\"Validation\", val), (\"Test\", test)]):\n",
    "    frame_counter = 0\n",
    "    for video in videos:\n",
    "        cap = cv2.VideoCapture(path_to_videos / video)\n",
    "        while True:\n",
    "            res, frame = cap.read()\n",
    "            if not res:\n",
    "                break\n",
    "            \n",
    "            # уменьшаем размер в 16 раз, 4к в данный момент нам не нужно\n",
    "            frame = cv2.resize(frame, None,fx=0.25, fy=0.25) \n",
    "            #Opencv зачем то повернуло видео на 90 градусов, разворачиваем обратно\n",
    "            #print(frame.shape)\n",
    "            frame = frame.transpose(1,0,2)[:,::-1,:].copy()\n",
    "\n",
    "            label_path = dataset_path / \"labels\" / name / f\"frame_{frame_counter:06d}.txt\"\n",
    "            labels = read_yolo_labels(label_path)\n",
    "\n",
    "            # Т.к у нас очень сильный data leakage перевернем изображения из тестового набора вертикально,\n",
    "            # чтобы иммитировать другой сценарий и убедится что модель не тупо заучила все возможные положения меток\n",
    "            if name in (\"Test\"):\n",
    "                frame, labels = flip_frame_and_labels(frame, labels, vertical=True)\n",
    "                save_yolo_labels(label_path, labels)\n",
    "\n",
    "            # используем каждый 5 кадр для ускорения экспериментов\n",
    "            if frame_counter % 5 == 0:\n",
    "                if not os.path.exists(dataset_path / \"images\" / name):\n",
    "                    os.makedirs(dataset_path / \"images\" / name)\n",
    "\n",
    "            cv2.imwrite(dataset_path / \"images\" / name / f\"frame_{frame_counter:06d}.png\", frame)\n",
    "\n",
    "            #Сразу отображаем экспортированные аннотации, что бы убедиться в корректности данных\n",
    "            draw_boxes(frame, labels, classes)\n",
    "\n",
    "            frame_counter +=1\n",
    "\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            key = cv2.waitKey(1)\n",
    "            if ord('q') == key:\n",
    "                break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c373c",
   "metadata": {},
   "source": [
    "# Baseline тренировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4f960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\") #nano модель что бы не переобучить\n",
    "\n",
    "# Запустим тренировку на 50 эпох что бы посмотреть как быстро модель переобучится\n",
    "model.train(\n",
    "    data=\"data/dataset/data.yaml\",  \n",
    "    epochs=50,                 \n",
    "    imgsz=640,                  \n",
    "    batch=32,               \n",
    "    project=\"Zebra-test\",      \n",
    "    name=\"experiments/baseline\",     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197a896",
   "metadata": {},
   "source": [
    "### Проверяем на отложенной выборке (видео перевернуто, для имитации другого сценария)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab55c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.159 🚀 Python-3.10.12 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3090, 24113MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 9625.8±2204.7 MB/s, size: 1087.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/maxim/Desktop/Zebra-Test/data/dataset/labels/Test.cache... 1728 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1728/1728 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 108/108 [00:07<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1728      23294      0.506      0.479      0.574      0.393\n",
      "                  dish       1728      10368      0.574      0.673      0.753      0.585\n",
      "                 drink       1728       6912      0.982      0.741      0.848      0.683\n",
      "            silverware       1728       3456      0.469      0.502      0.557       0.22\n",
      "         garbage/other       1378       2558          0          0      0.137     0.0823\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val17\u001b[0m\n",
      "0.5739212900116262\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"Zebra-test/experiments/baseline/weights/best.pt\")\n",
    "\n",
    "metrics = model.val(data=\"data/dataset/data.yaml\", split=\"test\")\n",
    "print(metrics.box.map50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46666f",
   "metadata": {},
   "source": [
    "### Визуализируем предсказания baseline на тестовом видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: results/baseline.mp4\n",
      "✅ Saved: results/baseline.mp4\n",
      "✅ Saved: results/baseline.mp4\n",
      "✅ Saved: results/baseline.mp4\n",
      "✅ Saved: results/baseline.mp4\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "video_paths = [\"data/videos_raw/1.MOV\",\"data/videos_raw/2_1.MOV\",\"data/videos_raw/3_1.MOV\",\"data/videos_raw/3_2.MOV\",\"data/videos_raw/4.MOV\"]\n",
    "output_path = \"results/baseline.mp4\"\n",
    "\n",
    "def save_video(model, video_paths, output_path):\n",
    "    out = None\n",
    "    for video in tqdm(video_paths):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        if not out:\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            # В обратном порядке, что бы перевернуть видео\n",
    "            width  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) // 2)\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) // 2)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.resize(frame, None,fx=0.5, fy=0.5) \n",
    "            frame = frame.transpose(1,0,2)[:,::-1,:].copy()\n",
    "            \n",
    "\n",
    "            results = model.predict(source=frame, stream=False, show=False, conf=0.25, verbose=False)[0]\n",
    "            annotated_frame = results.plot()\n",
    "            out.write(annotated_frame)\n",
    "\n",
    "        cap.release()\n",
    "    print(f\"✅ Saved: {output_path}\")\n",
    "    out.release()\n",
    "\n",
    "save_video(model, video_paths, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefa255",
   "metadata": {},
   "source": [
    "# Добавляем аугментаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все еще nano модель\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Так же я позволил себе явно указать используемый оптимайзер, и изменить learning rate\n",
    "# но забыл указать что это сделал в отчете\n",
    "model.train(\n",
    "    data=\"data/dataset/data.yaml\",  \n",
    "    epochs=50,                 \n",
    "    imgsz=640,                  \n",
    "    batch=32,\n",
    "    optimizer = \"AdamW\", \n",
    "    lr0 = 3e-4, #Karpatov magic constant for AdamW             \n",
    "    project=\"Zebra-test\",      \n",
    "    name=\"experiments/augs_tuned\", \n",
    "    warmup_epochs=3,\n",
    "    translate = 0.1,\n",
    "    mosaic= 0.3,\n",
    "    #close_mosaic= 3,\n",
    "    mixup= 0.2,\n",
    "    # Не используем отражение по вертикали, потому что это будет читерство в данном контексте\n",
    "    #flipud= 0.5, \n",
    "    fliplr= 0.5,\n",
    "    scale= 0.5,\n",
    "    copy_paste = 0.4,\n",
    "    erasing = 0.2,\n",
    "    shear = 15,\n",
    "    degrees= 90, \n",
    "    hsv_h = 0.1,\n",
    "    hsv_s = 0.1,\n",
    "    hsv_v = 0.5,\n",
    "    cos_lr = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f141c",
   "metadata": {},
   "source": [
    "### Проверяем на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eada76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.159 🚀 Python-3.10.12 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3090, 24113MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 10434.8±2157.4 MB/s, size: 1083.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/maxim/Desktop/Zebra-Test/data/dataset/labels/Test.cache... 1728 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1728/1728 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 108/108 [00:07<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1728      23294      0.674      0.846        0.8      0.538\n",
      "                  dish       1728      10368      0.744      0.997      0.988      0.711\n",
      "                 drink       1728       6912      0.643          1      0.995       0.76\n",
      "            silverware       1728       3456      0.401      0.979      0.699      0.314\n",
      "         garbage/other       1378       2558      0.906      0.409      0.519      0.366\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n",
      "0.800363749355843\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"Zebra-test/experiments/augs_tuned/weights/best.pt\")\n",
    "\n",
    "metrics = model.val(data=\"data/dataset/data.yaml\", split=\"test\")\n",
    "print(metrics.box.map50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa5220",
   "metadata": {},
   "source": [
    "### Сохраняем предсказания улучшенной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db3350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: results/tuned.mp4\n",
      "✅ Saved: results/tuned.mp4\n",
      "✅ Saved: results/tuned.mp4\n",
      "✅ Saved: results/tuned.mp4\n"
     ]
    }
   ],
   "source": [
    "save_video(model, video_paths, \"results/tuned.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf29118",
   "metadata": {},
   "source": [
    "# Бонус - как получить наивысшие метрики если вы вообще ничего не понимаете в ML\n",
    "### Создаем новый датасет из вообще всех размеченных изображений, 20% перекладываем в валидацию, 20% в тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "original_dataset = Path(\"data/dataset\")  # Путь к оригинальному датасету\n",
    "silly_dataset = Path(\"data/silly_dataset\")  # Куда складываем новый датасет\n",
    "silly_dataset.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Собираем все изображения\n",
    "all_images = list(original_dataset.glob(\"images/*/*.png\")) + \\\n",
    "             list(original_dataset.glob(\"images/*/*.jpg\")) + \\\n",
    "             list(original_dataset.glob(\"images/*/*.jpeg\"))\n",
    "\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# Разделение\n",
    "n = len(all_images)\n",
    "val_split = int(n * 0.2)\n",
    "test_split = int(n * 0.2)\n",
    "\n",
    "val_images = all_images[:val_split]\n",
    "test_images = all_images[val_split:val_split + test_split]\n",
    "train_images = all_images[val_split + test_split:]\n",
    "\n",
    "splits = {\n",
    "    \"Train\": train_images,\n",
    "    \"Validation\": val_images,\n",
    "    \"Test\": test_images\n",
    "}\n",
    "\n",
    "# Копируем изображения и соответствующие метки\n",
    "for split_name, image_list in splits.items():\n",
    "    image_dir = silly_dataset / \"images\" / split_name\n",
    "    label_dir = silly_dataset / \"labels\" / split_name\n",
    "    image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img_path in image_list:\n",
    "        # Копируем изображение\n",
    "        new_img_path = image_dir / img_path.name\n",
    "        shutil.copy(img_path, new_img_path)\n",
    "\n",
    "        # Соответствующий .txt файл\n",
    "        label_path = original_dataset / \"labels\" / img_path.parent.name / (img_path.stem + \".txt\")\n",
    "        if label_path.exists():\n",
    "            shutil.copy(label_path, label_dir / label_path.name)\n",
    "\n",
    "# Записываем новый data.yaml\n",
    "yaml_content = f\"\"\"path: {silly_dataset.resolve()}\n",
    "train: images/Train\n",
    "val: images/Validation\n",
    "test: images/Test\n",
    "\n",
    "names:\n",
    "  0: dish\n",
    "  1: drink\n",
    "  2: silverware\n",
    "  3: garbage/other\n",
    "\"\"\"\n",
    "\n",
    "with open(silly_dataset / \"data.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"✅ Silly dataset создан в:\", silly_dataset.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486f094",
   "metadata": {},
   "source": [
    "### Берем модель побольше, потому что больше - лучше)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=\"data/silly_dataset/data.yaml\",  \n",
    "    epochs=50,                 \n",
    "    imgsz=640,                  \n",
    "    batch=32,\n",
    "    optimizer = \"AdamW\",\n",
    "    lr0 = 3e-4, #Karpatov magic constant for AdamW             \n",
    "    project=\"Zebra-test\",      \n",
    "    name=\"experiments/silly\", \n",
    "    warmup_epochs=3,\n",
    "    translate = 0.1,\n",
    "    mosaic= 0.3,\n",
    "    #close_mosaic= 3,\n",
    "    mixup= 0.2,\n",
    "    #flipud= 0.5, # Не используем отражение по вертикали, потому что это будет читерство в данном контексте\n",
    "    fliplr= 0.5,\n",
    "    scale= 0.5,\n",
    "    copy_paste = 0.4,\n",
    "    erasing = 0.2,\n",
    "    shear = 15,\n",
    "    degrees= 90, \n",
    "    hsv_h = 0.1,\n",
    "    hsv_s = 0.1,\n",
    "    hsv_v = 0.5,\n",
    "    cos_lr = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af1a78",
   "metadata": {},
   "source": [
    "### Тестируем на отложенной выборке, которая пришла из того же распределения что и тренировочная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"Zebra-test/experiments/silly/weights/best.pt\")\n",
    "\n",
    "metrics = model.val(data=\"data/dataset/data.yaml\", split=\"test\")\n",
    "print(metrics.box.map50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140a14",
   "metadata": {},
   "source": [
    "### Наслаждаемся качеством ~~пере~~обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafe4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_video(model, video_path, \"results/overfitted.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
